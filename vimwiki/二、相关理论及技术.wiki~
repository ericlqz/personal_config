2.1 基于内容的过滤技术
    Collaborative Filtering Recommendations (协同过滤，简称CF) 是目前最流行的推荐方法，在研究界和工业界得到大量使用。但是，工业界真正使用的系统一般都不会只有CF推荐算法，Content-based Recommendations (CB) 基本也会是其中的一部分。
    CB的过程一般包括以下三步：
    1. Item Representation：为每个item抽取出一些特征（也就是item的content了）来表示此item；
    2. Profile Learning：利用一个用户过去喜欢（及不喜欢）的item的特征数据，来学习出此用户的喜好特征（profile）；
    3. Recommendation Generation：通过比较上一步得到的用户profile与候选item的特征，为此用户推荐一组相关性最大的item。
    *CB流程图*
2.1.1 Item Representation
    真实应用中的item往往都会有一些可以描述它的属性。这些属性通常可以分为两种：结构化的（structured）属性与非结构化的（unstructured）属性。所谓结构化的属性就是这个属性的意义比较明确，其取值限定在某个范围；而非结构化的属性往往其意义不太明确，取值也没什么限制，不好直接使用。比如在交友网站上，item就是人，一个item会有结构化属性如身高、学历、籍贯等，也会有非结构化属性（如item自己写的交友宣言，博客内容等等）。对于结构化数据，我们自然可以拿来就用；但对于非结构化数据（如文章），我们往往要先把它转化为结构化数据后才能在模型里加以使用。真实场景中碰到最多的非结构化数据可能就是文章了（如个性化阅读中）。下面我们就详细介绍下如何把非结构化的一篇文章结构化。
    如何代表一篇文章在信息检索中已经被研究了很多年了，下面介绍的表示技术其来源也是信息检索，其名称为向量空间模型（Vector Space Model，简称VSM）。
2.2.2  Profile Learning
    假设用户u已经对一些item给出了他的喜好判断，喜欢其中的一部分item，不喜欢其中的另一部分。那么，这一步要做的就是通过用户u过去的这些喜好判断，为他产生一个模型。有了这个模型，我们就可以根据此模型来判断用户u是否会喜欢一个新的item。所以，我们要解决的是一个典型的有监督分类问题，理论上机器学习里的分类算法都可以照搬进这里。
2.2.3 Recommendation Generation
    如果上一步Profile Learning中使用的是分类模型（如DT、LC和NB），那么我们只要把模型预测的用户最可能感兴趣的n个item作为推荐返回给用户即可。而如果Profile Learning中使用的直接学习用户属性的方法（如Rocchio算法），那么我们只要把与用户属性最相关的n个item作为推荐返回给用户即可。其中的用户属性与item属性的相关性可以使用如cosine等相似度度量获得
2.2.4 CB的优缺点
CB的优点：
1. 用户之间的独立性（User Independence）：既然每个用户的profile都是依据他本身对item的喜好获得的，自然就与他人的行为无关。而CF刚好相反，CF需要利用很多其他人的数据。CB的这种用户独立性带来的一个显著好处是别人不管对item如何作弊（比如利用多个账号把某个产品的排名刷上去）都不会影响到自己。
2. 好的可解释性（Transparency）：如果需要向用户解释为什么推荐了这些产品给他，你只要告诉他这些产品有某某属性，这些属性跟你的品味很匹配等等。
3. 新的item可以立刻得到推荐（New Item Problem）：只要一个新item加进item库，它就马上可以被推荐，被推荐的机会和老的item是一致的。而CF对于新item就很无奈，只有当此新item被某些用户喜欢过（或打过分），它才可能被推荐给其他用户。所以，如果一个纯CF的推荐系统，新加进来的item就永远不会被推荐:( 。
CB的缺点：
1. item的特征抽取一般很难（Limited Content Analysis）：如果系统中的item是文档（如个性化阅读中），那么我们现在可以比较容易地使用信息检索里的方法来“比较精确地”抽取出item的特征。但很多情况下我们很难从item中抽取出准确刻画item的特征，比如电影推荐中item是电影，社会化网络推荐中item是人，这些item属性都不好抽。其实，几乎在所有实际情况中我们抽取的item特征都仅能代表item的一些方面，不可能代表item的所有方面。这样带来的一个问题就是可能从两个item抽取出来的特征完全相同，这种情况下CB就完全无法区分这两个item了。比如如果只能从电影里抽取出演员、导演，那么两部有相同演员和导演的电影对于CB来说就完全不可区分了。
2. 无法挖掘出用户的潜在兴趣（Over-specialization）：既然CB的推荐只依赖于用户过去对某些item的喜好，它产生的推荐也都会和用户过去喜欢的item相似。如果一个人以前只看与推荐有关的文章，那CB只会给他推荐更多与推荐相关的文章，它不会知道用户可能还喜欢数码。
3. 无法为新用户产生推荐（New User Problem）：新用户没有喜好历史，自然无法获得他的profile，所以也就无法为他产生推荐了。当然，这个问题CF也有。
2.1 文本聚类
    聚类是一种无监督机器学习技术，是对点集进行考察并按照某种距离测度将它们聚成多个“簇”的过程。聚类的目标是使得同一簇内的点之间的距离较短，而不同簇中点之间的距离较大。对于文本聚类而言，同类的文档相似度较大，不同类的文档相似度较小。其中，相似度与距离为反比关系。文本聚类无需训练，而且不需要人工对文档进行类别标注，具有一定的灵活性和较高的自动化处理能力。
    文本聚类是文本挖掘和信息检索领域的核心问题之一，主要分为以下几个过程：文本预处理、文本建模、特征特取、取类等步骤。如下图所示：
    _图 文本聚类流程_
2.1.1 预处理
    (1). 中文分词
    中文分词在中文信息处理中是最基础的过程，无论机器翻译亦或信息检索还是其他相关应用，如果涉及中文，都离不开中文分词，作为中文信息处理的”桥头堡”，中文分词的重要性不言而喻。
    词是最小的能够独立活动的有意义的语言成分，英文单词之间以空格作为自然分界符，由于汉语自身的特点，词与词之间没有明显的分界，因此需要使用计算机自动对中文文本进行词语的切分，即像英文那样使得中文句子中的词之间有空格以标识。
    现有的分词算法可分为三大类：基于字符串匹配的分词方法、基于理解的分词方法和基于统计的分词方法。
　　1、基于字符串匹配的分词方法
　　这种方法又叫做机械分词方法，它是按照一定的策略将待分析的汉字串与一个“充分大的”机器词典中的词条进行配，若在词典中找到某个字符串，则匹配成功（识别出一个词）。按照扫描方向的不同，串匹配分词方法可以分为正向匹配和逆向匹配；按照不同长度优先匹配的情况，可以分为最大（最长）匹配和最小（最短）匹配；按照是否与词性标注过程相结合，又可以分为单纯分词方法和分词与标注相结合的一体化方法。常用的几种机械分词方法如下：
　　    1）正向最大匹配法（由左到右的方向）；
　　    2）逆向最大匹配法（由右到左的方向）；
　　    3）最少切分（使每一句中切出的词数最小）。
    2、基于理解的分词方法
　　这种分词方法是通过让计算机模拟人对句子的理解，达到识别词的效果。其基本思想就是在分词的同时进行句法、语义分析，利用句法信息和语义信息来处理歧义现象。它通常包括三个部分：分词子系统、句法语义子系统、总控部分。
    3、基于统计的分词方法
　　从形式上看，词是稳定的字的组合，因此在上下文中，相邻的字同时出现的次数越多，就越有可能构成一个词。因此字与字相邻共现的频率或概率能够较好的反映成词的可信度。可以对语料中相邻共现的各个字的组合的频度进行统计，计算它们的互现信息。定义两个字的互现信息，计算两个汉字X、Y的相邻共现概率。互现信息体现了汉字之间结合关系的紧密程度。当紧密程度高于某一个阈值时，便可认为此字组可能构成了一个词。这种方法只需对语料中的字组频度进行统计，不需要切分词典，因而又叫做无词典分词法或统计取词方法。但这种方法也有一定的局限性，会经常抽出一些共现频度高、但并不是词的常用字组
    针对各种不同的分词技术，已经开发出了不同的分词系统，本文采用中科院的ICTCLAS汉语词法分析系统。
    _ICTCLAS 介绍_
2.1.2 文本建模
    在IR(信息检索)领域，广泛使用VSM[6](向量空间模型)来表征文本的特征，即为文本建模。向量空间模型是由Salton等人于20世纪70年代提出，并成功运用到著名的SMART文本检索系统。VSM原理简单直观并在检索效率与性能方面有较好的平衡，成为经典并直到今天依然被广泛应用于信息过滤、信息检索、索引及关联规则等。
    VSM将文档映射到高维空间的向量上，每一维都对应了在整个文档集上中出现过的词项，从而将对文档内容的处理转化为向量空间中的向量运算，进而可以通过计算向量间的相似度来度量向量间的相似度。在实际应用中，最常使用的相似性度量方法为余弦距离。
    _图 余弦公式_
    _对公式的解释_
    对于文本建模而言，VSM将文档表示成如下的特征向量：
    _公式 VSM_
    在Salton, Wong和Yang提出的传统的向量空间模型中，使用TF-IDF[7]作为文档向量中特征项的权重。文档的权重向量如下所示：
    V,,d,, = [w,,1.d,,, w,,2.d,,...w,,n.d,,]^T^, 其中，
    w,,i.d = ... _公式 维基TFIDF_
    TFIDF函数可用于表示每个特征项区分文档的能力。tf-idf模型的主要思想是：如果词w在一篇文档d中出现的频率高，并且在其他文档中很少出现，则认为词w具有很好的区分能力，适合用来把文章d和其他文章区分开来。该模型主要包含了两个因素：
    1) 词w在文档d中的词频tf (Term Frequency)，即词w在文档d中出现次数count(w, d)和文档d中总词数size(d)的比值：tf(w,d) = count(w, d) / size(d) 
    2) 词w在整个文档集合中的逆向文档频率idf (Inverse Document Frequency)，即文档总数n与词w所出现文件数docs(w, D)比值的对数:idf = log(n / docs(w, D)) 
2.1.3 特征提取
    特征提取算法分为特征选择和特征抽取两大类，其中特征选择算法有互信息，文档频率，信息增益，x2统计量等十数种，在众多的特征选择方法中，有学者指出IG（信息增益）和CHI（x2 统计量）的效果最好[8]，而IG计算量相对其它几种方法较大，因此本文选用目前特征选择方法中效果最好的χ2统计方法。
    在文本分类的特征选择阶段，我们主要关心一个词t（一个随机变量）与一个类别c（另一个随机变量）之间是否相互独立？如果独立，就可以说词t对类别c完全没有表征作用，即我们根本无法根据t出现与否来判断一篇文档是否属于c这个分类。词t与类别c的开方值更一般的形式可以写成：
    _公式 卡方检验_  _基于LDA模型的文本聚类研究_
2.1.4 聚类
    [文本聚类是文本挖掘和信息检索领域的核心问题之一。依据著名的聚类假设：同类的文档相似度较大，不同类的文档相似度较小。根据文本文档的内容相关性来组织文档集合，将整个集合聚焦成若干个类，并使得同一类的文档尽量相似，属于不同类的文档差别明显。]文本聚类方法包含以下几囊：基于划分、基于层次、基于蚁群、基于网格等。
    基于划分的聚类算法应为较为普遍[]，在使用时，我们必须指定聚类的数目，然后反复迭代运算，逐次降低误差函数的值，直到误差函数不再变化。在所有基于划分的聚类算法中，最基本的方法，便是所谓的K-means算法，以称为Forgy's algorithm。其主要目标是在大量高维的文档集中找出最具有代表性的文档，这些文档称之为中心点，然后根据这些中心点，进行后续的归类及迭代计算。
    _K-mens算法流程图_
    _对K-means算法的评价_
2.2 主题模型
    近年来，主题模型（Topic Modeling）成为近年来文本挖掘领域的热点，它能够发现文档-词语之间所蕴含的潜在语义关系（即主题）——将文档看成一组主题的混合分布，而主题又是词语的概率分布——从而将高维度的“文档-词语”向量空间映射到低维度的“文档-主题”和“主题-词语”空间，有效提高了文本信息处理的性能。基于主题模型的文本情感分析技术，期望通过挖掘各种非结构化文本评论所蕴含的主题及其关联的情感特征，来提高文本情感分析的性能，也成为学术界（如UIC的Bing Liu，UIUC的Chengxiang Zhai等）和工业界（如Google）关注的热点。
    [主题模型（topic model）这个东东。何谓“主题”呢？望文生义就知道是什么意思了，就是诸如一篇文章、一段话、一个句子所表达的中心思想。不过从统计模型的角度来说， 我们是用一个特定的词频分布来刻画主题的，并认为一篇文章、一段话、一个句子是从一个概率模型中生成的。]
2.2.1 PLSA与LDA
    主题模型主要有两类：PLSA和LDA。如下图所示：主题模型假设每个文档由多个主题混合而成（利用文档在所有主题上的概率分布来表示），而每个主题都是词上的概率分布（即每个词对主题的贡献度），这样文档、词都可以映射到同一个潜在语义空间——主题。
    Hofmann等人于1999年提出一种基于概率的潜在语义分析（Probabilistic Latent Semantic Analysis）模型。PLSA继承了“潜在语义”的概念，通过“统一的潜在语义空间”（也就是Blei等人于2003年正式提出Topic概念）来关联词与文档；通过引入概率统计的思想，避免了SVD的复杂计算。在PLSA中，各个因素（文档、潜在语义空间、词）之间的概率分布求解是最重要的，EM算法是常用的方法。PLSA也存在一些缺点：概率模型不够完备；随着文档和词的个数的增加，模型变得越来越庞大；在文档层面没有一个统计模型；EM算法需要反复迭代，计算量也很大。
    鉴于PLSA的缺点，Blei等人于2003年进一步提出新的主题模型LDA（Latent Dirichlet Allocation），它是一个层次贝叶斯模型，把模型的参数也看作随机变量，从而可以引入控制参数的参数，实现彻底的“概率化”。
    LDA是一种非监督机器学习技术，可以用来识别大规模文档集（document collection）或语料库（corpus）中潜藏的主题信息。它采用了词袋（bag of words）的方法，这种方法将每一篇文档视为一个词频向量，从而将文本信息转化为了易于建模的数字信息。但是词袋方法没有考虑词与词之间的顺序，这简化了问题的复杂性，同时也为模型的改进提供了契机。每一篇文档代表了一些主题所构成的一个概率分布，而每一个主题又代表了很多单词所构成的一个概率分布。
2.2.2 主题模型的优点
    1.它可以衡量文档之间的语义相似性。对于一篇文档，我们求出来的主题分布可以看作是对它的一个抽象表示。对于概率分布，我们可以通过一些距离公式（比如KL距离）来计算出两篇文档的语义距离，从而得到它们之间的相似度。
    2.它可以解决多义词的问题。回想最开始的例子，“苹果”可能是水果，也可能指苹果公司。通过我们求出来的“词语－主题”概率分布，我们就可以知道“苹果”都属于哪些主题，就可以通过主题的匹配来计算它与其他文字之间的相似度。
    3.它可以排除文档中噪音的影响。一般来说，文档中的噪音往往处于次要主题中，我们可以把它们忽略掉，只保持文档中最主要的主题。
    4.它是无监督的，完全自动化的。我们只需要提供训练文档，它就可以自动训练出各种概率，无需任何人工标注过程。
    5.它是跟语言无关的。任何语言只要能够对它进行分词，就可以进行训练，得到它的主题分布。
2.3 朴素贝叶斯分类器
    朴素贝叶斯分类器是一种应用基于独立假设的贝叶斯定理的简单概率分类器.更精确的描述这种潜在的概率模型为独立特征模型。
    分类是将一个未知样本分到几个预先已知类的过程。数据分类问题的解决是一个两步过程：第一步,建立一个模型，描述预先的数据集或概念集。通过分析由属性描述的样本（或实例，对象等）来构造模型。假定每一个样本都有一个预先定义的类，由一个被称为类标签的属性确定。为建立模型而被分析的数据元组形成训练数据集，该步也称作有指导的学习。
    在众多的分类模型中，应用最为广泛的两种分类模型是决策树模型(Decision Tree Model)和朴素贝叶斯模型（Naive Bayesian Model，NBC）。决策树模型通过构造树来解决分类问题。首先利用训练数据集来构造一棵决策树，一旦树建立起来，它就可为未知样本产生一个分类。在分 类问题中使用决策树模型有很多的优点，决策树便于使用，而且高效；根据决策树可以很容易地构造出规则，而规则通常易于解释和理解；决策树可很好地扩展到大 型数据库中，同时它的大小独立于数据库的大小；决策树模型的另外一大优点就是可以对有许多属性的数据集构造决策树。决策树模型也有一些缺点，比如处理缺失 数据时的困难，过度拟合问题的出现，以及忽略数据集中属性之间的相关性等。
　　和决策树模型相比，朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以 及稳定的分类效率。同时，NBC模型所需估计的参数很少，对缺失数据不太敏感，算法也比较简单。理论上，NBC模型与其他分类方法相比具有最小的误差率。 但是实际上并非总是如此，这是因为NBC模型假设属性之间相互独立，这个假设在实际应用中往往是不成立的，这给NBC模型的正确分类带来了一定影响。在属 性个数比较多或者属性之间相关性较大时，NBC模型的分类效率比不上决策树模型。而在属性相关性较小时，NBC模型的性能最为良好。
    尽管实际上独立假设常常是不准确的，但朴素贝叶斯分类器的若干特性让其在实践中能够取得令人惊奇的效果。特别地，各类条件特征之间的解耦意味着每个特征的分布都可以独立地被当做一维分布来估计。这样减轻了由于维数灾带来的阻碍,当样本的特征个数增加时就不需要使样本规模呈指数增长。然而朴素贝叶斯在大多数情况下不能对类概率做出非常准确的估计，但在许多应用中这一点并不要求。例如，朴素贝叶斯分类器中，依据最大后验概率决策规则只要正确类的后验概率比其他类要高就可以得到正确的分类。所以不管概率估计轻度的甚至是严重的不精确都不影响正确的分类结果。在这种方式下，分类器可以有足够的鲁棒性去忽略朴素贝叶斯概率模型上存在的缺陷。
    _NB 公式_
    _NB 公式解析_
    


*基于内容的过滤*
*中文分词*
*文本建模*
    日常生活中总是产生大量的文本，如果每一篇文本存储为一篇文档，那每篇文档从人的观察来说就是有序的词的序列d=(w,,1,, , w,,2,, , ..., w,,n,,)。
    _图包含m篇文档的语料库 LDA Math_
1. VSM文本建模
2. 统计文本建模
    统计文本建模的目的就是追问这些观察到语料库中的词序列是如何生成的。人类产生的所有语料文本都可以看成是以某一种方式概率生成的。
2.1 Unigram Model
2.2 PLSA
    Unigram Model是一个很简单的模型，模型中的假设过于简单，和人类写文章产生每一个词的过程差距比较大。Hoffmn于1999年提出PLSA(Probabilistic Latent Semantic Analysis)模型。认为一篇文章是由多个主题混合而成，而每个主题都是词汇上的概率分布。
    ××××
    PLSA中文档和文档之间是独立可交换的，同一个文档内的词也是中交换的，是一个词袋模型。所以求解PLSA这个Topic Model的过程汇总，模型参数容易求解，可以使用著名的EM算法进行求得局部最优解。
2.3 LDA
*特征选择*
*文本聚类*
*分类器*
