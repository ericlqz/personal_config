*3.1 需求分析*
    对于基于内容的推荐技术而言，其推荐的效果依赖于对所推荐物品的有效建模，即如何抽取最能精确刻画所推荐物品的特征。
    从做一个好的推荐系统的角度来看，和领域知识的结合是密不可分的。然而，目前我们所接触到的较为普遍的做法是比较重算法，而忽略了之前的领域问题分析和对数据的把握。一般是准备好数据（抽取成经典的user-item矩阵）之后，选几个比较经典通用的算法(user/item-KNN, 矩阵分解等)跑一跑，选个好的结果。这样做比较方便快捷，但往往因为忽略了领域自身的特点，而错过了很多改进的机会。
    在资讯领域中，常常使用信息检索领域的VSM模型(文档的词频指标)来表示文本建模，但对于用户而言，资讯是否符合用户的兴趣偏好，往往是由资讯的类别亦或其主题决定的。也就是说，单纯地根据文档中词频这些外部指标来判断文档是否符合用户的兴趣是不够的，我们需要挖掘出文档的内部信息，更加深入完整地刻画文档的特征。
    本文使用LDA主题模型挖掘文档的内部主题信息，采用文本聚类获取文本的类别信息，并结合文档的主题信息及分类信息作为文档的特征模型。
*3.2 详细设计*
3.2.1 文档数据集约定
    在本章文本建模的设计实现过程所使用的文档数据集，按照以下要求存储：
    1. 文档只包含一篇资讯的正文内容;
    2. 存在若干个大的分类，如CLASS1, ... CLASSN，每个分类表现为一个文件夹，将文档按类别存入相应的文件夹中。
    在数据的处理过程中，需要先对文档进行分类处理，再做聚类。因为数据的噪声会在分类之后得到很大的抑制，聚类的效果就会非常好。如果反过来的话，就会出现比如科技新闻划分到时事类新闻等情况。
    一个比较简单的分类思路就是爬取资讯的时候就做好分类，从相应的资讯源的某一分类中爬取的资讯自然就归到这个分类中，这种分类方法完全依靠专家分类，分类的准确性会很高。但这种办法不能用在所有的数据源，因为很多的资讯并没有分类，或者来自不同的资讯源。但是，这些专家分类的数据源可以作为分类器的训练集。
    在推荐系统中，完整的数据获取过程包括：
    1. 使用爬虫从资讯源抓取信息，然后对抓取来的不同格式的信息进行解析，"清洗"数据，获取资讯网页的文本及图片，并对这些文本进行创建索引的工作。
    2. 对文本进行分类;
    3. 通过聚类技术对文本进行分组;
    其中，数据的爬取、解析、索引，及数据的自动分类两个模块也是另外的两个研究课题，具有相当的研究及工作量，不在本文的研究范围内。
3.2.2 文本聚类
    文本聚类用于将文档集聚集成若干个类别。本文在文档集已有分类的基础上，对每个分类中的所有文档进行操作，获得文档的二级分类，亦可称为分组。
    文本聚类除了可以获取文档的二级分类信息外，还具有以下优点：
    1. 在资讯推荐领域中，用户数是远远小于文档数的，在这种情况下，我们很难学习出用户对特定网页的喜好，计算量太大，此时，聚类是最有效的。我们可以先对网页聚类，然后学习出用户对不同类型网页的态度。
    2. 用户一般都是同时关注多个资讯源，不同的资讯源有着自己定制的人工的分类信息，对于资讯推荐系统而言，我们无法整合不同来源的形式各异的分类信息，这就要求推荐系统具有对资讯进行自动文本聚类的功能。
    3. 已有分类下的文档可能包含着数量庞大的资讯，这个大的分类不足以作为文档的特征。
    在本文的文本聚类过程中，使用了CHI特征选择方法。特征选择方法是有监督的在文本分类上取得了成功，有效地选择出与分类信息最相关的特征项。但文本聚类是一种无监督的学习方法，缺少特征选择所必须的办公设备上，因此很少应用到文本聚类中。本文采用类似于EM算法思想的方法将特征选择方法应用到文本聚类中。  
3.2.3 LDA建模
    [向量空间模型在内容数据丰富时可以获得比较好的效果。以资讯文本为例，如果是计算长文本的相似度，用向量空间模型利用关键词计算相似度已经可以获得很高的精确度。但是，如果文本很短，关键词很少，向量空间模型就很难计算出的相似度。假设有两篇资讯，标题分别为"计算机的发展历程"及"微机的革命"，二者虽然关键词不同，但所表述的内容是类似的。换句话说，这两篇文章的关键词虽然不同，但关键词所属的主题是相同的。在这种情况下，首先需要知道文章的话题分布，然后才能准确地计算文章的相似度。研究重点便在于如何建立文章、话题和关键词关系的主题模型。代表的主题模型有LDA。]
    通过LDA模型，我们可以挖掘出文档的内部主题信息，用于文档的特征表示。
    [任何模型都有一个假设，LDA作为一种生成模型，对一篇文档产生的过程进行了建模。主题模型的基本思想是，一个人在写一篇文档的时候，会首先想这篇文章要讨论哪些话题，然后思考这些话题应该用什么词描述，从而最终用词写成一篇文章。因此文章和词之间是通过话题联系的。]
    可以很自然地看到，一篇文章通常是由多个主题构砀、而每一个主题大概可以用与该主题相关的频率最高的一些词来描述。主题模型认为一篇文档(Document)可以由多个主题(Topic)混合而成，而每个Topic都是词汇上的概率分布，文章中的每个词都是由一个固定的Topic生成的。
    _图3.1 LDA三层关系_
    在LDA模型中，文档的生成规则如下：
    1. 假定存在两种类型的分布，
        a. 一类是doc-topic分布，每一个doc-topic分布对应着一篇可能存在的文档，每个doc-topic有K个取值，每个取值是一个topic的编号。
        b. 一类是topic-word分布，每一个topic-word分布对应着一个可能存在的主题，每个toppic-word有V个取值，每个取值对应一个词。
    2. 随机独立抽取K个topic-word分布，编号为1-K。
    3. 每次生成一篇新文档前，随机抽取一个doc-topic分布，然后重复如下过程生成文档中的词：
        a. 随机获取这个doc-topic分布的一个值，得到一个topic编号z;
        b. 选择K个topic-word分布中编号为z的分布，随机获取这个分布的一个值，可以得到一个词。
    _图3.2 LDA的文档生成过程_
3.2.4 文本建模
    对于一个文档，可以通过LDA模型将文档映射到隐含的主题中，从而将文档看作一组隐含主题的混合。我们将文档d的主题向量记为Td = [t1,t2,...tn]。
    为了获取文档的类别信息，需采用分类器计算文档对于已存在的聚类结果的相关性。设文档d的类别向量记为Cd = [w1,w2,...wk]。
    我们将文档的主题向量与类别向量线性加权，作为文档的特征向量。
*3.3 具体实现* 
    本项目的实现代码以Python为主。
3.3.1 分词处理
    作为中文自然语言处理的"桥头堡"，中文分词是文本聚类及LDA建模必须的预处理过程之一。 本文采用中科院的ICTLAS系统实现分词功能。
    ICTCLAS（Institute of Computing Technology,Chinese Lexical Analysis System），由中国科学院计算技术研究开发，功能包括中文分词；词性标注；命名实体识别；新词识别；同时支持用户词典；支持繁体中文；支持gb2312、GBK、UTF8等多种编码格式，是世界上最好的汉语词法分析器之一。
    原系统只提供了C++和Java版本，本项目基于Linux环境下的Python2.7对其调用进行封装。
    对于输入的字符串，分词程序将返回形式为[word1, word2, ... wordN]的词语列表。
3.3.2 文档聚类
    文档聚类旨在对已有分类下的所有文档进一步细分，得到文档的二级类别信息。对于数据集中的所有类目，循环执行以下步骤：
    _图3.3 文本聚类流程图_
    1. 预处理
    为了便于后续的处理流程，我们将类目下待处理的资讯文档集合成到一个文件中。其中，文件中的每一行代表一个文档。如行19代表文档19。
    遍历类目下的资讯文档集D，通过文件IO读取文档的内容，将文档中的换行符替换为空白符，并将替换后的字符串写入文件中。通过这一操作，输出文件DATA.dat中包含了待处理资讯集列表。
    2. VSM建模及特征矩阵
    [一般来说，推荐的内容可以通过VSM(向量空间模型)表示，该模型会将推荐物品表示成一个关键词向量。如果物品的内容是一些诸如导演、演员(如电影)等实体的话，可以直接将这些实体作为关键词。但对于资讯而言，其内容以文本的方式展现，则需要引入一些自然语言处理技术来抽取关键词。对于中文，首先要对文本进行分词，将字流变成词流，然后从词流中检测出命名实体(如人名等)，这些实体和一些其他重要的词将组成关键词集合，最后对关键词进行排名，计算每个关键词的权重，从而生成关键词向量。
    _图3.4 关键词向量的生成过程_
    对于资讯文档d，其内容表示成一个关键词向量如下：
    d,,i,, = {(e,,1,, , w,,1,,), (e,,2,, , w,,2,,),...}
    其中，e,,i,,就是关键词，w,,i,,是关键词对应的权重。对于资讯而言，我们可以用信息检索领域著名的TF_IDF公式计算词的权重。
    w,,i,, = TF(e,,i,,) / logDF(e,,i,,)
    向量空间模型的优点是简单，缺点是丢失了一些信息，比如关键词之间的关系信息。不过在绝大多数应用中，向量空间模型对于文本的分类、聚类、相似度计算已经可以给出令人满意的结果。]

    在VSM建模过程中，使用TF_IDF作为文本特征项的权重。同时，在对资讯的处理过程中，会记录文本集中所出现的所有词语(term)、这些词语所对应的ID、每个词语的TF、IDF等信息。
    以预处理的输出文件DATA.dat作为输入数据。顺序读取文件中的每一行数据(即每一篇资讯)，应用分词处理程序获取当篇资讯的词语(term)列表。顺序读取资讯的词语(term)列表，记录文本集中出现过的所有词语(term)，并为其分配ID，计算在每一篇文档中词语出现的次数TF，以及词语的逆文档频率IDF。基于存储空间的考虑，同时也因为在形成特征矩阵后的计算中，都不是针对文档的词语(term)进行直接计算，而是针对词语(term)所对应的ID。
    在数据读取完毕后，将上述与词语相关的信息写入文件中。此时，可以获得每一篇文档的采用TF_IDF的SVM特征向量。比如，对于文档d，其特征向量如下所示：SVMd = [w1, w2, ..., wn]，其中n表示文档集中所有词语(term)的数目，w1表示词语1的TF_IDF值。
    文档集的特征向量则构成了特征矩阵，矩阵的行数为文档的个数，列数为文档集中所有词语的个数，矩阵中每个值都是TF_IDF值。经过构建特征矩阵这个步骤后，文本就转化成了数学语言（矩阵）了，之后的算法都是运行在矩阵之上，不再关心输入的数据是否是文档了。
    3. 文本聚类-特征提取
    _K-means算法简述，常用场景，简单评价_
    本文采用K-menas算法对文本进行聚类。 k均值算法的计算过程非常直观：
      a. 从D中随机取k篇资讯，作为k个簇的各自的中心。
      b. 分别计算剩下的资讯到k个簇中心的相异度，将这些元素分别划归到相异度最低的簇。
      c. 根据聚类结果，重新计算k个簇各自的中心。
      d. 将D中全部资讯按照新的中心重新聚类。
      e. 重复第4步，直到聚类结果收敛不再变化。
      f. 将结果输出。[公式http://www.cnblogs.com/jerrylead/archive/2011/04/06/2006910.html]
    其中，相异度指的是两篇资讯的差异程度。使用Eaulidean Distance公式来计算聚簇中心。
    _公式 Eaulidean Distance_
    为了进一步提高聚类的效果，本文将CHI特征提取应用到K-means算法中，具体过程如下:
    [1. 执行第一遍K-means算法，获得初始聚类和各类的聚类中心；  2 对步骤1生成的数据集，利用CHIR，进行特征选择。被选择的特征将继续保持在表示文本集的向量空间中，而没有被选择的那些特征，其乘以一个特定的权重值f，f为提前设定的一个因子，取值范围为（0,1），从而减少他们对于文本聚类的影响力。紧接着，在新的向量空间中，重新计算K个类的聚类中心。  3 文档集中的每一篇文档都要利用聚类标准函数来与新的向量空间中各个类的中心进行比较，从而将每一篇文档划分到与其最为相似的类别中。  4 反复迭代步骤2和步骤3，直到簇中心不发生改变。]
    4. 聚类结果的保存
    聚类结果输出文件、term_to_id、id_to_term、id_to_doc_count、class_to_doc_count、id_to_idf
    _图3.5 聚类二级类目产生_
3.3.3 LDA主题分布
    使用LDA主题模型对文本进行统计建模，获取"文档-主题-特征词"的三层贝叶斯模型。本文采用GibbsLDA++实现LDA建模。GibbsLDA++是基于C/C++实现的LDA模型，采用Gibbs Sampling技术应用于模型的参数估计及新文档的主题模型推断。该实现方案运行效率高，适用于分析大规模语料的潜在主题结构。
    _GibbsLDA++建模过程 文本聚类论文_
    1. 预处理
    对训练语料库中的文本进行分词处理，并按照gibbslda++的训练语料的格式要求创建lda模型建模的输入文件lda-data.dat。 *lda模型与大规模语料的关系*
    顺序读取语料库中的所有文本，对每个文本进行中文分词，将返回的词语列表按照"每篇文本一行，每行中的词语列表以空格作为间隔"的格式写入到输出文件中LDA-DATA.dat中，最后在文件头部插入一行，记录语料库中的文本总数。
    _图3.6 LDA输入文件格式_
    2. 训练LDA建模
    使用概率图模型表示，LDA模型的训练过程如图所示：
    _图3.7 LDA概率图模型_
    可以将LDA建模分解为两个主要的物理过程：
      a. a->0,,m,,->z,,(mn),,，这个过程表示在生成第m篇文档的时候，先随机抽取一个doc-topic分布0,,m,,， 然后随机获取该分布的一个值，即生成了文档中第n个词的topic编号z,,(mn),,;
      b. b->r,,k,,->w,,mn,,|k=z,,mn,,，这个过程表示用如下动作生成语料中第m篇文档的第n个词：在K个topic-word分布中，挑选编号为k=z,,mn,,的那个分布进行随机取值，然后生成word w,,mn,,;
    在步骤a中，语料中的M篇文档的topics生成过程是相互独立的，从而可以得到整个语料中topics生成概率。p(z|a) = ....
    在步骤b中，语料中的K个topics生成words的过程是相互独立的，从而可以得到整个语料中词的生成概率。p(w|z,b) = ...
    综合公式*与*，我们××× p(w,z|a,b)...
    在上式中，我们的目的是获取矩阵z及w;
    *参数估计的...*
    
    在实现过程，我们通过python调用GibbsLDA++的可执行程序进行LDA文本建模，并传入以下参数作为模型训练的参数。
    lda -est [-alpha <double>] [-beta <double>] [-ntopics <int>] [-niters <int>] [-savesteps <int>] [-twords <int>] -dfile <string>
    其中：
    -est: 表示将要进行的操作是训练LDA模型
    -alpha：表示LDA模型中文档-主题分布的先验分布，默认的值是50/K(K为主题数)
    -beta：表示LDA模型中主题-词语分布的先验分布，默认的值是0.1
    -ntopics：欲分析的主题数，默认值为100
    -niters：表示Gibbs Sampling的迭代数，默认值为2000;
    -savesteps：指明LDA模型应该在哪些迭代次数时保存起来，默认值是200;
    -twords：表示为每个潜在主题分配多少个与该主题最相关的词语;
    -dfile：指明训练数据的路径;
    3. 输出模型分析
    完成LDA模型的训练过程后，可以得到下图所示的输出结果，这些输出结果是LDA模型的一种展现方式。
    <model_name>.others
    <model_name>.phi
    <model_name>.theta
    <model_name>.tassign
    <model_name>.twords
    wordmap.txt
    其中：
    <model_name>是在各迭代数保存的LDA模型的名字。如在第400次Gibbs Sampling迭代时保存的模型名将会是model-00400。在最后一次迭代完成后保存的LDA模型的名字为model-final。
    <model_name>.others文件包含了LDA模型训练过程中的所有参数。比如：
        alpha=?
        beta=?
        ntopics=?   # 主题数
        ndocs=?     # 文档数
        nwords=?    # 训练文档集中词表的大小
        liter=?     # 模型是在Gibbs Sampling的哪一次迭代过程中保存的
    <model_name>.phi包含了主题-词语分布，如p(word,,w,,|topic,,t,,)。文件中每一行表示一个主题，每一列表示一个单词
    <model_name>.theta包含了文档-主题分布，如p(topic,,t,,|document,,m,,)。文件中每一行表示一个文档，每一列表示一个主题。
    <model_name>.tassign包含了在训练数据中，每一文档中每个词语的主题分布。文件中每一行表示一个文档，每一列表示文档中该位置的单词分配的主题。
    <model_name>.twords包含了每个主题中与该主题最相关的若干个词语。文件中每一行表示一个主题，每一列表示与该主题最相关的词语之一。
    wordmap.txt包含了训练文档集中所有的词语及词语ID的映射。
3.3.4 文本分类
    在上文的文本聚类过程中，我们对每一个类目下的文档集进行了聚类操作，从而获得了该类别下更为精确的二级类别信息。在这些聚类结果的基础上，我们将计算每一份资讯文档的类别向量，即文档与各聚类的相关性。如Cd = (w,,c1,,, w,,c2,,...w,,ck,,)，其中k为聚类结果的类别个数，w,,ci,,指文档d与聚类i的相关性。并将文档的类别向量作为文档特征向量的组成部分。
    在已有类别的情况下，计算文档与各类别的相关性，这是一个典型的有监督分类问题。一个分类问题的解决包含两部分：训练分类器、使用分类器进行分类。
    贝叶斯分类方法是最常用的分类算法之一，本文采用朴素贝叶斯分类器来进行文本分类操作，朴素贝叶斯器运行效率高，且往往具有不错的分类效果。
    朴素贝叶斯分类器的工作流程如下：
    ~~朴素贝叶斯分类器的工作流程~~
    _图3.8 文本分类流程图_
    *CHI特征选择简述* 
    1. 预处理
    与在文本聚类过程中对资讯数据的预处理类似，将资讯数据进行分词、替换换行符为空白符等操作后写入到训练数据文件Cluster-Train-Data.dat文件中; 与之前操作不同的是，在每一行的行末加上此资讯的聚类类别信息。如下图所示：
    _图3.9 文本分类训练数据格式_
    2. 训练文本分类器
    读入训练数据文件，进行VSM文本建模，并构建训练数据的特征矩阵。详细过程已在文本聚类的处理过程中介绍，此处不再重复。
    对特征矩阵进行CHI特征选择，采用max特征选择方法，保留与类别相关度最高的20%的特征项，并输出特征选择的日志信息，包括每个特征项计算出的类别相关性、保留的特征项及过滤的特征项，并且将CHI特征选择模型保存到chi-filter.modle文件中。
    使用卡方检测算法来看看哪些特征（词）是没有什么必要的，并且将其过滤掉，其Create方法传入的参数就是之前的trainx, trainy，结果将是一个保存在chiFilter实例中的一个黑名单，表示哪些term-id是需要过滤掉的。其TestFilter方法就是使用之前得到的黑名单来过滤掉矩阵中不重要的列。
    对优化后的特征矩阵，进行贝叶斯分类器训练过程。
    首先，通过训练数据中的分类信息，计算每个分类的先验概率;
    然后，对于特征矩阵中的每一个特征项，计算特征项对每一个分类的极大似然概率;
    分类的先验概率以及特征项对每一个分类的极大似然概率组合得到了朴素贝叶斯分类器的分类模型，将分类模型保存到naive_bayes.model文件中。
    3. 使用分类器进行分类
    完成了分类器的训练工作后，我们便可以使用分类器来对新的单个文档或文档集进行分类操作。
    对一个文档的分类工作如下：
    a. 对文档进行中文分词;
    b. 根据已保存的CHI特征选择模型对文档进行特征过滤，仅保留文档中与类别最相关的20%特征项;
    c. 根据已保存的朴素贝叶斯分类器模型对文档的特征向量进行分类计算，获得文档与各类别的相关性。比如：Cd = (w,,c1,,, w,,c2,,...w,,ck,,)。
3.3.5 文本建模
    本文将文档的主题向量及分类向量结合，用于表征文本的特征向量。文本建模的流程如下图所示：
    _图3.10 线性加权流程图_
    下面介绍对单个文档建模的过程，对于文档集的建模过程与此类似。
    1. 预处理
    对文档进行中文分词操作，获取文档的特征词序列。按照gibbslda++的训练语料的格式要求创建lda模型的输入文件lda-data.dat；
    2. 获取文档主题向量
    使用已保存的LDA模型对文档进行主题分布分析:
    lda -inf -dir <string> -model <string> [-niters <int>] [-twords <int>] -dfile <string>
    其中：
    -inf：表示使用已训练过的LDA模型对新数据进行主题分析;
    -dir：包含已训练过的LDA模型的路径;
    -model：要使用的已训练的LDA模型的名称;
    -niters：Gibbs Sampling参数分析要进行的迭代运算次数;
    -twords：表示为每个潜在主题分配多少个与该主题最相关的词语;
    -dfile：指明训练数据的路径;
    从<model_name>.theta文件中读取文档的主题向量, Td = ***
    3. 获取文档类别向量
    过程3.3.4中通过朴素贝叶斯分类器为文档进行分类操作，输出文档的类别向量Cd = ***
    4. 文本建模
    我们将文档的主题向量与类别向量线性加权，作为文档的特征向量。即d = (a*Td,(1-a)Cd)
